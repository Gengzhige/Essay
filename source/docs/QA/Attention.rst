Attention
---------

**如果去掉rnn的传递的连接关系变为self-attention机制，您的页面特别像MLP的结构，我不懂这两者之间有什么核心的区别。**

   内部结构完全不一样，MLP每一层是线性运算加激活函数，attention结构是qkv运算加残差网络，正则化，前馈网络。
